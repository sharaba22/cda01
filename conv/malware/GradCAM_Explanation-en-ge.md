
## ğŸ‡¬ğŸ‡§ English Version

Grad-CAM (Gradient-weighted Class Activation Mapping) helps us **visualize which regions** of the input are most influential for a given prediction (e.g., malware detection).

### 1ï¸âƒ£ Last Convolutional Layer

The last Conv2D layer outputs a tensor:

$A \in \mathbb{R}^{H \times W \times K}$

- **H, W** â€” spatial dimensions (e.g., 16Ã—16)
- **K** â€” number of channels (filters)

Each channel $A^k$ is an **activation map** showing where that filter â€œrespondsâ€ on the image.

### 2ï¸âƒ£ Gradient Weights

For each channel, Grad-CAM computes its influence on the class score:

$\alpha_k = \frac{1}{H \times W} \sum_{i,j} \frac{\partial y^{\text{malware}}}{\partial A^k_{ij}}$

$\alpha_k$ represents how much increasing that channel boosts the malware probability.

### 3ï¸âƒ£ Combining the Channels

The heatmap is obtained by the weighted sum:

$L_{Grad-CAM}(i,j) = ReLU\left( \sum_k \alpha_k A^k_{ij} \right)$

ReLU keeps only the regions that have a positive contribution.

### 4ï¸âƒ£ Normalization and Resizing

The heatmap is normalized to [0,1] and resized to the original image size.

### 5ï¸âƒ£ Visualization

The heatmap is **overlaid** on the grayscale image (`alpha=0.4`), where:
- **Red/Yellow** â€” areas most influential for the prediction;
- **Blue/Cold** â€” less or not influential regions.

### 6ï¸âƒ£ Intuitive Table

| Step | Operation | Meaning |
|------|------------|----------|
| Conv2D output | $A^k$ | Feature map per filter |
| Gradient | $\frac{\partial y}{\partial A^k}$ | Importance of channel |
| Mean gradient | $\alpha_k$ | Weight for each channel |
| Weighted sum | $\sum_k \alpha_k A^k$ | Combined influence map |
| ReLU + resize | | Keeps only positive influence |
| Overlay | | Makes result human-interpretable |

---

**Summary:**  
Grad-CAM creates a â€œthermal mapâ€ that highlights **which regions most strongly influenced the CNNâ€™s decision**, giving human insight into what the model â€œlooked at.â€

---


# Grad-CAM áƒ¡áƒ˜áƒ—áƒ‘áƒ£áƒ áƒ˜ áƒ áƒ£áƒ™áƒ˜áƒ¡ áƒáƒ áƒ˜áƒœáƒªáƒ˜áƒáƒ˜ / Grad-CAM Heatmap Principle

## ğŸ‡¬ğŸ‡ª áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ

Grad-CAM (Gradient-weighted Class Activation Mapping) áƒ’áƒ•áƒáƒ«áƒšáƒ”áƒ•áƒ¡ áƒ¡áƒáƒ¨áƒ£áƒáƒšáƒ”áƒ‘áƒáƒ¡ áƒ“áƒáƒ•áƒ˜áƒœáƒáƒ®áƒáƒ—, áƒ—áƒ£ áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒ£áƒ‘áƒœáƒ”áƒ‘áƒ˜áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ—áƒ•áƒáƒšáƒ¡áƒáƒ–áƒ áƒ˜áƒ¡áƒ˜áƒ— áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ•áƒáƒœáƒ˜ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ (áƒ›áƒáƒ’., áƒ›áƒáƒšáƒ•áƒ”áƒáƒ áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒœáƒáƒ‘áƒ˜áƒ¡áƒáƒ¡).

### 1ï¸âƒ£ áƒ‘áƒáƒšáƒ áƒ™áƒáƒœáƒ•áƒáƒšáƒ£áƒªáƒ˜áƒ£áƒ áƒ˜ áƒ¤áƒ”áƒœáƒ

áƒ‘áƒáƒšáƒ Conv2D áƒ¤áƒ”áƒœáƒ áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ¡ áƒ¢áƒ”áƒœáƒ–áƒáƒ áƒ¡:

$A \in \mathbb{R}^{H \times W \times K}$

- **H, W** â€” áƒ¡áƒ˜áƒ•áƒ áƒªáƒ˜áƒ—áƒ˜ áƒ–áƒáƒ›áƒ”áƒ‘áƒ˜áƒ (áƒ›áƒáƒ’. 16Ã—16)
- **K** â€” áƒáƒ áƒ®áƒ”áƒ‘áƒ˜áƒ¡ (áƒ¤áƒ˜áƒšáƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡) áƒ áƒáƒáƒ“áƒ”áƒœáƒáƒ‘áƒ

áƒ—áƒ˜áƒ—áƒ áƒáƒ áƒ®áƒ˜ $A^k$ áƒ¬áƒáƒ áƒ›áƒáƒáƒ“áƒ’áƒ”áƒœáƒ¡ áƒ”áƒ áƒ— **áƒáƒ¥áƒ¢áƒ˜áƒ•áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ¢áƒ áƒ˜áƒªáƒáƒ¡**, áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª áƒáƒ¡áƒáƒ®áƒáƒ•áƒ¡, áƒ¡áƒáƒ“ "áƒ áƒ”áƒáƒ’áƒ˜áƒ áƒ”áƒ‘áƒ¡" áƒ¤áƒ˜áƒšáƒ¢áƒ áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ–áƒ”.

### 2ï¸âƒ£ áƒ’áƒ áƒáƒ“áƒ˜áƒ”áƒœáƒ¢áƒ£áƒ áƒ˜ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜

áƒ—áƒ˜áƒ—áƒ áƒáƒ áƒ®áƒ˜áƒ¡ áƒ’áƒáƒ•áƒšáƒ”áƒœáƒ áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ–áƒ” áƒ˜áƒ–áƒáƒ›áƒ”áƒ‘áƒ áƒ’áƒ áƒáƒ“áƒ˜áƒ”áƒœáƒ¢áƒ˜áƒ—:

$\alpha_k = \frac{1}{H \times W} \sum_{i,j} \frac{\partial y^{\text{malware}}}{\partial A^k_{ij}}$

$\alpha_k$ áƒ’áƒ•áƒ˜áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ¡, áƒ—áƒ£ áƒ áƒáƒ›áƒ“áƒ”áƒœáƒáƒ“ áƒ–áƒ áƒ“áƒ˜áƒ¡ áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ¡ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒáƒ áƒ®áƒ˜áƒ¡ áƒ’áƒáƒ«áƒšáƒ˜áƒ”áƒ áƒ”áƒ‘áƒ.

### 3ï¸âƒ£ áƒáƒ áƒ®áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ áƒ¬áƒ§áƒ›áƒ

áƒ’áƒáƒ“áƒ›áƒáƒ•áƒ˜áƒ¢áƒáƒœáƒáƒ— áƒ”áƒ¡ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜ áƒáƒ áƒ®áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ¥áƒ¢áƒ˜áƒ•áƒáƒªáƒ˜áƒ”áƒ‘áƒ–áƒ”:

$L_{Grad-CAM}(i,j) = ReLU\left( \sum_k \alpha_k A^k_{ij} \right)$

áƒ”áƒ¡ áƒ’áƒáƒ”áƒ áƒ—áƒ˜áƒáƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ áƒ£áƒ™áƒ áƒ’áƒ•áƒ˜áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ¡, áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒ£áƒ‘áƒœáƒ”áƒ‘áƒ˜ áƒáƒ®áƒ“áƒ”áƒœáƒ”áƒœ áƒáƒáƒ–áƒ˜áƒ¢áƒ˜áƒ£áƒ  áƒ’áƒáƒ•áƒšáƒ”áƒœáƒáƒ¡ áƒ¡áƒáƒ‘áƒáƒšáƒáƒ áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ–áƒ”.

### 4ï¸âƒ£ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ“áƒ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ

áƒ áƒ£áƒ™áƒ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒ“áƒ”áƒ‘áƒ 0â€“1 áƒ“áƒ˜áƒáƒáƒáƒ–áƒáƒœáƒ¨áƒ˜ áƒ“áƒ áƒ’áƒáƒ“áƒ˜áƒ“áƒ“áƒ”áƒ‘áƒ áƒ¡áƒáƒ¬áƒ§áƒ˜áƒ¡áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ–áƒáƒ›áƒáƒ›áƒ“áƒ”.

### 5ï¸âƒ£ áƒ•áƒ˜áƒ–áƒ£áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ

áƒ—áƒ‘áƒ£áƒ áƒ˜ áƒ áƒ£áƒ™áƒ áƒ—áƒáƒ•áƒ¡áƒ“áƒ”áƒ‘áƒ áƒ’áƒ áƒáƒ“áƒ˜áƒ”áƒœáƒ¢áƒ£áƒš (áƒ¨áƒáƒ•áƒ˜áƒ¡ áƒ“áƒ áƒ—áƒ”áƒ—áƒ áƒ˜áƒ¡ áƒ’áƒ áƒáƒ“áƒ˜áƒ”áƒœáƒ¢áƒ˜) áƒ¡áƒ£áƒ áƒáƒ—áƒ–áƒ” áƒ’áƒáƒ›áƒ­áƒ•áƒ˜áƒ áƒ•áƒáƒšáƒáƒ‘áƒ˜áƒ— (`alpha=0.4`), áƒ¡áƒáƒ“áƒáƒª:
- **áƒ¬áƒ˜áƒ—áƒ”áƒšáƒ˜/áƒ§áƒ•áƒ˜áƒ—áƒ”áƒšáƒ˜** áƒ£áƒ‘áƒœáƒ”áƒ‘áƒ˜ â€” áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ•áƒáƒœáƒ˜áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡;
- **áƒšáƒ£áƒ áƒ¯áƒ˜/áƒªáƒ˜áƒ•áƒ˜** áƒ£áƒ‘áƒœáƒ”áƒ‘áƒ˜ â€” áƒ£áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒ áƒáƒœ áƒœáƒ”áƒ˜áƒ¢áƒ áƒáƒšáƒ£áƒ áƒ˜.

### 6ï¸âƒ£ áƒ˜áƒœáƒ¢áƒ£áƒ˜áƒªáƒ˜áƒ£áƒ áƒáƒ“

| áƒ”áƒ¢áƒáƒáƒ˜ | áƒ›áƒáƒ¥áƒ›áƒ”áƒ“áƒ”áƒ‘áƒ | áƒáƒ–áƒ áƒ˜ |
|-------|------------|------|
| Conv2D output | $A^k$ | áƒ—áƒ˜áƒ—áƒ áƒ¤áƒ˜áƒšáƒ¢áƒ áƒ˜áƒ¡ áƒœáƒáƒœáƒáƒ®áƒ˜ áƒœáƒ˜áƒ›áƒ£áƒ¨áƒ˜ |
| Gradient | $\frac{\partial y}{\partial A^k}$ | áƒ áƒáƒ›áƒ”áƒšáƒ˜ áƒáƒ áƒ®áƒ”áƒ‘áƒ˜áƒ áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ•áƒáƒœáƒ˜ |
| Mean gradient | $\alpha_k$ | áƒáƒ áƒ®áƒ˜áƒ¡ áƒ¬áƒáƒœáƒ |
| Weighted sum | $\sum_k \alpha_k A^k$ | áƒ¡áƒáƒ”áƒ áƒ—áƒ áƒ’áƒáƒ•áƒšáƒ”áƒœáƒ˜áƒ¡ áƒ áƒ£áƒ™áƒ |
| ReLU + resize | | áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ“áƒáƒ“áƒ”áƒ‘áƒ˜áƒ—áƒ˜ áƒ–áƒáƒœáƒ”áƒ‘áƒ˜ |
| Overlay | | áƒáƒ“áƒáƒ›áƒ˜áƒáƒœáƒ˜áƒ¡ áƒ›áƒ˜áƒ”áƒ  áƒáƒ¦áƒ¥áƒ›áƒáƒ“áƒ˜áƒ áƒ¤áƒ”áƒ áƒ”áƒ‘áƒáƒ“ |

