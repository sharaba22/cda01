
## 🇬🇧 English Version

Grad-CAM (Gradient-weighted Class Activation Mapping) helps us **visualize which regions** of the input are most influential for a given prediction (e.g., malware detection).

### 1️⃣ Last Convolutional Layer

The last Conv2D layer outputs a tensor:

$A \in \mathbb{R}^{H \times W \times K}$

- **H, W** — spatial dimensions (e.g., 16×16)
- **K** — number of channels (filters)

Each channel $A^k$ is an **activation map** showing where that filter “responds” on the image.

### 2️⃣ Gradient Weights

For each channel, Grad-CAM computes its influence on the class score:

$\alpha_k = \frac{1}{H \times W} \sum_{i,j} \frac{\partial y^{\text{malware}}}{\partial A^k_{ij}}$

$\alpha_k$ represents how much increasing that channel boosts the malware probability.

### 3️⃣ Combining the Channels

The heatmap is obtained by the weighted sum:

$L_{Grad-CAM}(i,j) = ReLU\left( \sum_k \alpha_k A^k_{ij} \right)$

ReLU keeps only the regions that have a positive contribution.

### 4️⃣ Normalization and Resizing

The heatmap is normalized to [0,1] and resized to the original image size.

### 5️⃣ Visualization

The heatmap is **overlaid** on the grayscale image (`alpha=0.4`), where:
- **Red/Yellow** — areas most influential for the prediction;
- **Blue/Cold** — less or not influential regions.

### 6️⃣ Intuitive Table

| Step | Operation | Meaning |
|------|------------|----------|
| Conv2D output | $A^k$ | Feature map per filter |
| Gradient | $\frac{\partial y}{\partial A^k}$ | Importance of channel |
| Mean gradient | $\alpha_k$ | Weight for each channel |
| Weighted sum | $\sum_k \alpha_k A^k$ | Combined influence map |
| ReLU + resize | | Keeps only positive influence |
| Overlay | | Makes result human-interpretable |

---

**Summary:**  
Grad-CAM creates a “thermal map” that highlights **which regions most strongly influenced the CNN’s decision**, giving human insight into what the model “looked at.”

---


# Grad-CAM სითბური რუკის პრინციპი / Grad-CAM Heatmap Principle

## 🇬🇪 ქართული ვერსია

Grad-CAM (Gradient-weighted Class Activation Mapping) გვაძლევს საშუალებას დავინახოთ, თუ რომელი უბნებია მოდელის თვალსაზრისით ყველაზე მნიშვნელოვანი კონკრეტული პროგნოზისთვის (მაგ., მალვეარის ამოცნობისას).

### 1️⃣ ბოლო კონვოლუციური ფენა

ბოლო Conv2D ფენა აბრუნებს ტენზორს:

$A \in \mathbb{R}^{H \times W \times K}$

- **H, W** — სივრცითი ზომებია (მაგ. 16×16)
- **K** — არხების (ფილტრების) რაოდენობა

თითო არხი $A^k$ წარმოადგენს ერთ **აქტივაციის მატრიცას**, რომელიც ასახავს, სად "რეაგირებს" ფილტრი სურათზე.

### 2️⃣ გრადიენტური წონები

თითო არხის გავლენა პროგნოზზე იზომება გრადიენტით:

$\alpha_k = \frac{1}{H \times W} \sum_{i,j} \frac{\partial y^{\text{malware}}}{\partial A^k_{ij}}$

$\alpha_k$ გვიჩვენებს, თუ რამდენად ზრდის პროგნოზს კონკრეტული არხის გაძლიერება.

### 3️⃣ არხების შერწყმა

გადმოვიტანოთ ეს წონები არხების აქტივაციებზე:

$L_{Grad-CAM}(i,j) = ReLU\left( \sum_k \alpha_k A^k_{ij} \right)$

ეს გაერთიანებული რუკა გვიჩვენებს, რომელი უბნები ახდენენ პოზიტიურ გავლენას საბოლოო პროგნოზზე.

### 4️⃣ ნორმალიზაცია და გაფართოება

რუკა ნორმალიზდება 0–1 დიაპაზონში და გადიდდება საწყისი სურათის ზომამდე.

### 5️⃣ ვიზუალიზაცია

თბური რუკა თავსდება გრადიენტულ (შავის და თეთრის გრადიენტი) სურათზე გამჭვირვალობით (`alpha=0.4`), სადაც:
- **წითელი/ყვითელი** უბნები — ყველაზე მნიშვნელოვანია მოდელისთვის;
- **ლურჯი/ცივი** უბნები — უმნიშვნელო ან ნეიტრალური.

### 6️⃣ ინტუიციურად

| ეტაპი | მოქმედება | აზრი |
|-------|------------|------|
| Conv2D output | $A^k$ | თითო ფილტრის ნანახი ნიმუში |
| Gradient | $\frac{\partial y}{\partial A^k}$ | რომელი არხებია მნიშვნელოვანი |
| Mean gradient | $\alpha_k$ | არხის წონა |
| Weighted sum | $\sum_k \alpha_k A^k$ | საერთო გავლენის რუკა |
| ReLU + resize | | მხოლოდ დადებითი ზონები |
| Overlay | | ადამიანის მიერ აღქმადია ფერებად |

